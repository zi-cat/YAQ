{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Scraper for the AITA subreddit created using PRAW and PSAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scraper produces JSON files of submissions from the AITA subreddit. Each JSON file contains a list of dictionaries with relevant information about the submission for my analysis: post title, text, label, upvotes, upvote ratio, id and URL. I initially used this to scrape 100k AITA posts \n",
    "\n",
    "I had to combine PRAW and PSAW because some of the specific reddit submission search fields that I needed (link_flair_text, score) were not accurate in PSAW, but PRAW's time-range search functionality has been disabled. This combined scraper first performs a time-range search on the AITA subreddit using PSAW. The submission ids from those results are then put intp PRAW, which mines the relevant submission information.  \n",
    "\n",
    "PRAW (https://praw.readthedocs.io/en/latest/) and PSAW were both used to create this scraper. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import praw\n",
    "reddit = praw.Reddit(client_id='YOUR_ID', client_secret='YOUR_SECRET', user_agent='YOUR_AGENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "api = PushshiftAPI()\n",
    "subreddit = reddit.subreddit('AmItheAsshole')\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_posts(start = datetime.date(2020,1,14), lim=1000):\n",
    "    \n",
    "    #initialize post list\n",
    "    posts = []        \n",
    "    \n",
    "    #search for posts from before the specified time in psaw\n",
    "    results = list(api.search_submissions(before=start,\n",
    "                                subreddit='amitheasshole', #change subreddit\n",
    "                                filter=['url','num_comments','created_utc','id'], #change traits returned\n",
    "                                limit=lim))      \n",
    "    \n",
    "    for i in results:\n",
    "        \n",
    "        #insert the id of the results into PRAW\n",
    "        j = praw.models.Submission(reddit,id=i.id)\n",
    "    \n",
    "        #add results to list if link_flair_text meets criteria\n",
    "        if j.link_flair_text!=None:\n",
    "            post_dict = {}\n",
    "            post_dict[\"title\"] = j.title\n",
    "            post_dict[\"text\"] = j.selftext\n",
    "            post_dict[\"label\"] = j.link_flair_text\n",
    "            post_dict['upvotes'] = j.score\n",
    "            post_dict['upvote_ratio'] = j.upvote_ratio\n",
    "            post_dict['id'] = j.id\n",
    "            post_dict['url'] = j.url\n",
    "            posts.append(post_dict)\n",
    "    \n",
    "    #return list of posts and the timestamp of the last post in the search\n",
    "    return posts, results[-1].created_utc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 10000 #the number of posts you want to acquire\n",
    "post_list = [] #list of posts\n",
    "time = datetime.date(2020,1,14) #start date\n",
    "\n",
    "for i in range(10000000000000000000000000000): #an arbitrarily large number for the range so it doesn't stop before it needs to\n",
    "    \n",
    "    if len(post_list)<target: #continue using the get_more_posts function until post_list is long enough\n",
    "        \n",
    "        print(time) #optional for seeing when a new loop starts\n",
    "        \n",
    "        (posts,time) = get_more_posts(time,1000)\n",
    "        post_list.extend(posts)\n",
    "        \n",
    "        with open('aita_psaw_praw_{}.json'.format(str(len(post_list))), 'w') as json_file: #optional, save files from one iteration\n",
    "            json.dump(post_list, json_file)\n",
    "    \n",
    "    elif len(post_list)>target:\n",
    "        \n",
    "        with open('aita_psaw_praw_final_{}.json'.format(str(len(post_list))), 'w') as json_file: #save final file\n",
    "            json.dump(post_list, json_file)\n",
    "            \n",
    "        with open(\"final_time.json\",'w') as json_file: #save final timestamp in case you need to run some more\n",
    "            json.dump(time,json_file)\n",
    "            \n",
    "        print(time) #optional for seeing the last timestamp\n",
    "        \n",
    "        break #exit the loop\n",
    "        \n",
    "    else: # in case you can't hit the target for whatever reason\n",
    "        \n",
    "        with open('aita_psaw_praw_final_{}.json'.format(str(len(post_list))), 'w') as json_file: #save final file\n",
    "            json.dump(post_list, json_file)\n",
    "            \n",
    "        with open(\"final_time.json\",'w') as json_file: #save final timestamp in case you need to run some more\n",
    "            json.dump(time,json_file)\n",
    "            \n",
    "        print(time) #optional for seeing the last timestamp\n",
    "        \n",
    "        break #exit the loop\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
